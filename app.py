from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.chains import RetrievalQA
from chains.prompt_templates import get_prompt_template
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai.chat_models import ChatOpenAI


embedding = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
#é…ç½®api keyä½¿ç”¨å‘½ä»¤ï¼šexport OPENAI_API_KEY="sk-xxxxxxxxxxxxxxxx"
#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

# åŠ è½½æœ¬åœ°å‘é‡åº“
vectorstore = FAISS.load_local("vectorstore/faiss_index", embeddings=embedding, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever(search_kwargs={"k": 4})

# åŠ è½½å¤§æ¨¡å‹ï¼ˆè¿™é‡Œä»ç„¶æ˜¯ OpenAI çš„ GPT-4ï¼Œä½ å¯ä»¥æŒ‰éœ€æ›¿æ¢ï¼‰
llm = ChatOpenAI(model="gpt-4", temperature=0.2, api_key="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")

# è‡ªå®šä¹‰ Prompt æ¨¡æ¿
prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=get_prompt_template()
)

# æ„å»º RAG QA chain
rag_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    chain_type="stuff",
    chain_type_kwargs={"prompt": prompt}
)

# ç”¨æˆ·æé—®å¾ªç¯
while True:
    query = input("\nAsk a science question (or 'exit'): ")
    if query.lower() == "exit":
        break
    try:
        answer = rag_chain.run(query)
        print(f"\nğŸ§  Answer:\n{answer}")
    except Exception as e:
        print(f"âŒ Error: {e}")

