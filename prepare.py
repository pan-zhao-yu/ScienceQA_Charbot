from datasets import load_dataset
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from tqdm import tqdm
import os
import shutil

os.environ["TOKENIZERS_PARALLELISM"] = "false" #åœæ­¢ä¸€äº›ä¸å¿…è¦çš„è­¦å‘Š
# åŠ è½½æ•°æ®ï¼ˆå­—å…¸ç»“æ„ï¼Œtrain split æ˜¯ dict çš„ valuesï¼‰
dataset = load_dataset("derek-thomas/ScienceQA")
train_data = dataset["train"]
print(f"ğŸ“¦ Loaded {len(train_data)} training examples")

# æ„å»º LangChain Documents
documents = []
for example in tqdm(train_data):
    question = example.get("question", "").replace("\n", " ").strip().lower()
    choices = [c.replace("\n", " ").strip().lower() for c in example.get("choices", [])]
    answer = example.get("answer", "")
    lecture = example.get("lecture", "")
    hint = example.get("hint", "")
    solution = example.get("solution", "")
    subject = example.get("subject", "")
    topic = example.get("topic", "")
    skill = example.get("skill", "")
    category = example.get("category", "")
    task_type = example.get("task", "")

    # ä»…ç”¨äºembeddingçš„å†…å®¹ï¼ŒåªåŒ…å«questionå’Œchoices
    embedding_content = f"[Question] {question}[Choices] {', '.join(choices)}"

    # å®Œæ•´ä¿¡æ¯å­˜å…¥metadataä¸­
    full_content = f"""
[Question] {question}
[Choices] {', '.join(choices)}
[Answer] {answer}
[Lecture] {lecture}
[Hint] {hint}
[Solution] {solution}
[Category] {category} | Subject: {subject} | Topic: {topic} | Skill: {skill}
    """.strip()

    metadata = {
        "full_content": full_content,
        "subject": subject,
        "topic": topic,
        "skill": skill,
        "task": task_type,
        "category": category,
    }

    documents.append(Document(page_content=embedding_content, metadata=metadata))

#Chunking åˆ†å—æ–‡æœ¬ï¼ˆæŒ‰éœ€ä¿®æ”¹å‚æ•°ï¼‰
splitter = RecursiveCharacterTextSplitter(chunk_size=10000, chunk_overlap=0) #one doc per chunk
split_docs = splitter.split_documents(documents)
print(f"âœ‚ï¸ Split into {len(split_docs)} chunks")

# åµŒå…¥æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")


# è¾“å‡ºå‰5ä¸ªæ–‡æ¡£çš„ embedding content å’Œå¯¹åº”çš„ embedding å‘é‡
print("embedding æ–‡æ¡£åŠå…¶å‘é‡ï¼š")
for doc in split_docs[:20]:
    # è¿™é‡Œè°ƒç”¨ embed_query ç”Ÿæˆå‘é‡ï¼ˆæ³¨æ„è¾“å…¥å¿…é¡»ä¸æ„é€ æ—¶ä¸€è‡´ï¼‰
    vector = embedding_model.embed_query(doc.page_content)
    print("Embedding Content:")
    print(doc.page_content)
    # print("Embedding Vector:")
    # print(vector)
    print("-" * 50)


# æ¸…ç†å·²æœ‰è·¯å¾„
save_path = "vectorstore/faiss_index"
if os.path.isfile(save_path):
    os.remove(save_path)
if os.path.isdir(save_path):
    shutil.rmtree(save_path)


# æ„å»º FAISS å‘é‡åº“
vectorstore = FAISS.from_documents(split_docs, embedding_model)
print("FAISS vectorstore created")

# ä¿å­˜å‘é‡åº“
os.makedirs("vectorstore", exist_ok=True)
vectorstore.save_local(save_path)
print(f"Vector store saved at {save_path}")