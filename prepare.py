from datasets import load_dataset
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from tqdm import tqdm
import os
import shutil

# åœæ­¢éƒ¨åˆ†ä¸å¿…è¦çš„è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# åŠ è½½ ScienceQA æ•°æ®é›†
dataset = load_dataset("derek-thomas/ScienceQA")
train_data = dataset["train"]
print(f"ğŸ“¦ Loaded {len(train_data)} training examples")

# æ„å»º LangChain æ–‡æ¡£
documents = []
for example in tqdm(train_data):
    question = example.get("question", "")
    choices = example.get("choices", [])
    lecture = example.get("lecture", "")
    hint = example.get("hint", "")
    solution = example.get("solution", "")
    subject = example.get("subject", "")
    topic = example.get("topic", "")
    skill = example.get("skill", "")
    category = example.get("category", "")
    task_type = example.get("task", "")
    answer_index = example.get("answer", None)
    image = example.get("image", "")

    # æ„é€ æ›´åŠ ç»“æ„åŒ–ã€è¯¦ç»†çš„æ–‡æ¡£å†…å®¹ï¼Œè¿™æ ·èƒ½ä¿è¯åµŒå…¥æ—¶è¦†ç›–æ›´å…¨é¢çš„ä¿¡æ¯
    content = f"""
ã€Subjectã€‘: {subject}    ã€Topicã€‘: {topic}    ã€Skillã€‘: {skill}    ã€Categoryã€‘: {category}
ã€Taskã€‘: {task_type}    ã€Answer Indexã€‘: {answer_index}
-------------------------------
[Question]
{question}

[Choices]
{', '.join(choices)}

-------------------------------
[Lecture]
{lecture}

-------------------------------
[Hint]
{hint}

-------------------------------
[Solution]
{solution}
    """.strip()

    # å¢åŠ  metadata ä¿¡æ¯ä»¥ä¾¿åç»­æ£€ç´¢æˆ–è¾…åŠ©ç­›é€‰
    metadata = {
        "subject": subject,
        "topic": topic,
        "skill": skill,
        "task": task_type,
        "category": category,
        "answer": answer_index,
        "image": image,
    }

    documents.append(Document(page_content=content, metadata=metadata))

# ä½¿ç”¨è¾ƒå°çš„ chunk_size ä»¥åŠé€‚å½“å¢åŠ  chunk_overlap æ¥ç¡®ä¿æ¯ä¸ª chunk å†…èƒ½ä¿ç•™å®Œæ•´çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
split_docs = splitter.split_documents(documents)
print(f"âœ‚ï¸ Split into {len(split_docs)} chunks")

# åˆå§‹åŒ–æœ¬åœ°åµŒå…¥æ¨¡å‹ï¼ˆä½¿ç”¨ HuggingFace çš„ sentence-transformers æ¨¡å‹ï¼‰
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# æ¸…ç†å·²æœ‰çš„å‘é‡åº“æ–‡ä»¶å¤¹
save_path = "vectorstore/faiss_index"
if os.path.isfile(save_path):
    os.remove(save_path)
if os.path.isdir(save_path):
    shutil.rmtree(save_path)

# ä½¿ç”¨ FAISS æ„å»ºå‘é‡åº“
vectorstore = FAISS.from_documents(split_docs, embedding_model)
print("FAISS vectorstore created")

# ä¿å­˜å‘é‡åº“
os.makedirs("vectorstore", exist_ok=True)
vectorstore.save_local(save_path)
print(f"Vector store saved at {save_path}")
