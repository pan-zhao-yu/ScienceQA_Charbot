from datasets import load_dataset
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
from tqdm import tqdm
import os
import shutil

os.environ["TOKENIZERS_PARALLELISM"] = "false"
# åŠ è½½æ•°æ®ï¼ˆå­—å…¸ç»“æ„ï¼Œtrain split æ˜¯ dict çš„ valuesï¼‰
dataset = load_dataset("derek-thomas/ScienceQA")
train_data = dataset["train"]
print(f"ğŸ“¦ Loaded {len(train_data)} training examples")

# æ„å»º LangChain Documents
documents = []
for example in tqdm(train_data):
    question = example.get("question", "")
    choices = example.get("choices", [])
    lecture = example.get("lecture", "")
    hint = example.get("hint", "")
    solution = example.get("solution", "")
    subject = example.get("subject", "")
    topic = example.get("topic", "")
    skill = example.get("skill", "")
    category = example.get("category", "")
    task_type = example.get("task", "")

    # æ‹¼æ¥æˆä¸€ä¸ª chunk çš„å†…å®¹
    content = f"""
[Question] {question}
[Choices] {', '.join(choices)}
[Lecture] {lecture}
[Hint] {hint}
[Solution] {solution}
[Category] {category} | Subject: {subject} | Topic: {topic} | Skill: {skill}
"""

    metadata = {
        "subject": subject,
        "topic": topic,
        "skill": skill,
        "task": task_type,
        "category": category
    }

    documents.append(Document(page_content=content.strip(), metadata=metadata))

# åˆ†å—æ–‡æœ¬ï¼ˆå¯æŒ‰éœ€ä¿®æ”¹å‚æ•°ï¼‰
splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
split_docs = splitter.split_documents(documents)
print(f"âœ‚ï¸ Split into {len(split_docs)} chunks")

# åµŒå…¥æ¨¡å‹ï¼ˆæœ¬åœ°ï¼‰
embedding_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

# æ¸…ç†å·²æœ‰è·¯å¾„
save_path = "vectorstore/faiss_index"
if os.path.isfile(save_path):
    os.remove(save_path)
if os.path.isdir(save_path):
    shutil.rmtree(save_path)


# æ„å»º FAISS å‘é‡åº“
vectorstore = FAISS.from_documents(split_docs, embedding_model)
print("âœ… FAISS vectorstore created")

# ä¿å­˜å‘é‡åº“
os.makedirs("vectorstore", exist_ok=True)
vectorstore.save_local(save_path)
print(f"âœ… Vector store saved at {save_path}")
